{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQZF1KNn60V3fVa6aNK6P8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1.  What is a Decision Tree, and how does it work in the context of\n","classification?**\n","\n","A Decision Tree is a supervised learning algorithm used for both classification and regression tasks — but it’s most commonly used for classification.\n","\n","**2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n","How do they impact the splits in a Decision Tree?**\n","\n","Gini Impurity measures how often a randomly chosen sample would be misclassified, while Entropy measures the randomness or disorder in a node — both guide Decision Trees to choose splits that create purer child nodes.\n","\n","**3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n","Trees? Give one practical advantage of using each.**\n","\n","Pre-Pruning stops the tree from growing too deep by applying constraints (like max depth, min samples per leaf) during training, preventing overfitting early.\n","Post-Pruning allows the tree to grow fully and then cuts back less important branches based on validation performance.\n","\n","**4.What is Information Gain in Decision Trees, and why is it important for\n","choosing the best split?**\n","\n","Information Gain measures how much uncertainty (entropy) is reduced after splitting a dataset based on a feature.\n","\n","**5.What are some common real-world applications of Decision Trees, and\n","what are their main advantages and limitations?**\n","\n","Decision Trees are used in areas like credit scoring, medical diagnosis, and fraud detection; they’re easy to interpret and handle mixed data types but can overfit and be sensitive to small data changes.\n","\n","**10. Imagine you’re working as a data scientist for a healthcare company that\n","wants to predict whether a patient has a certain disease. You have a large dataset with\n","mixed data types and some missing values.\n","Explain the step-by-step process you would follow to:\n","● Handle the missing values\n","● Encode the categorical features\n","● Train a Decision Tree model\n","● Tune its hyperparameters\n","● Evaluate its performance\n","And describe what business value this model could provide in the real-world\n","setting.**\n","\n","To build a disease prediction model, first handle missing values by imputing numerical features with the median and categorical ones with the mode (or use advanced imputers if needed). Then encode categorical data using one-hot encoding for nominal features and ordinal encoding for ordered ones. Next, train a Decision Tree classifier within a preprocessing pipeline that includes imputation and encoding. Use GridSearchCV or RandomizedSearchCV to tune key hyperparameters such as max_depth, min_samples_split, and criterion based on cross-validation results. Finally, evaluate the model using metrics like accuracy, precision, recall, F1-score, and ROC-AUC on a test set.\n","In practice, this model helps healthcare providers predict diseases early, prioritize high-risk patients, and optimize medical resources, improving both patient outcomes and operational efficiency.\n"],"metadata":{"id":"hBJKPBgOUy2S"}},{"cell_type":"code","source":["'''6. Write a Python program to:\n","● Load the Iris Dataset\n","● Train a Decision Tree Classifier using the Gini criterion\n","● Print the model’s accuracy and feature importances'''\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","X, y = load_iris(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","model = DecisionTreeClassifier(criterion='gini', random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Feature Importances:\", model.feature_importances_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZGJQKHtV49g","executionInfo":{"status":"ok","timestamp":1762083378250,"user_tz":-330,"elapsed":2679,"user":{"displayName":"rakesh kumar","userId":"00511206985775184146"}},"outputId":"2a897ed2-cc00-4f44-f657-874869e623df"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n","Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"]}]},{"cell_type":"code","source":["''' 7:  Write a Python program to:\n","● Load the Iris Dataset\n","● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n","a fully-grown tree.'''\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","X, y = load_iris(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n","tree_limited.fit(X_train, y_train)\n","y_pred_limited = tree_limited.predict(X_test)\n","\n","tree_full = DecisionTreeClassifier(random_state=42)\n","tree_full.fit(X_train, y_train)\n","y_pred_full = tree_full.predict(X_test)\n","\n","print(\"Accuracy (max_depth=3):\", accuracy_score(y_test, y_pred_limited))\n","print(\"Accuracy (fully-grown):\", accuracy_score(y_test, y_pred_full))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPTS-MBYWmjp","executionInfo":{"status":"ok","timestamp":1762083488748,"user_tz":-330,"elapsed":55,"user":{"displayName":"rakesh kumar","userId":"00511206985775184146"}},"outputId":"73e36bb2-35c8-4d97-ada2-cdcedf77545e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (max_depth=3): 1.0\n","Accuracy (fully-grown): 1.0\n"]}]},{"cell_type":"code","source":["''' 8: Write a Python program to:\n","● Load the Boston Housing Dataset\n","● Train a Decision Tree Regressor\n","● Print the Mean Squared Error (MSE) and feature importances'''\n","\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","X, y = fetch_california_housing(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","model = DecisionTreeRegressor(random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n","print(\"Feature Importances:\", model.feature_importances_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-AlOEAVXEKW","executionInfo":{"status":"ok","timestamp":1762083527850,"user_tz":-330,"elapsed":2796,"user":{"displayName":"rakesh kumar","userId":"00511206985775184146"}},"outputId":"d3470246-e0ce-4a99-c5b1-24a380ce6c18"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 0.5280096503174904\n","Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n"," 0.08999238 0.08880639]\n"]}]},{"cell_type":"code","source":["'''9: Write a Python program to:\n","● Load the Iris Dataset\n","● Tune the Decision Tree’s max_depth and min_samples_split using\n","GridSearchCV\n","● Print the best parameters and the resulting model accuracy'''\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","X, y = load_iris(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","param_grid = {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 3, 4, 5, 10]}\n","grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n","grid.fit(X_train, y_train)\n","\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","print(\"Best Parameters:\", grid.best_params_)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aeJPPPdXNCJ","executionInfo":{"status":"ok","timestamp":1762083586776,"user_tz":-330,"elapsed":456,"user":{"displayName":"rakesh kumar","userId":"00511206985775184146"}},"outputId":"5764dd87-f648-481a-84e3-c050e41f0ce3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n","Accuracy: 1.0\n"]}]}]}